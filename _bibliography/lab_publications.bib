
@inproceedings{sixt_when_2020,
	title = {When {Explanations} {Lie}: {Why} {Many} {Modified} {BP} {Attributions} {Fail}},
	volume = {1},
	shorttitle = {When {Explanations} {Lie}},
	url = {https://proceedings.icml.cc/paper/2020/hash/af21d0c97db2e27e13572cbf59eb343d},
	abstract = {Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically. For code see: https://github.com/berleon/when-explanations-lie},
	language = {en},
	urldate = {2020-09-17},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	author = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
	year = {2020},
	keywords = {peer-reviewed},
	file = {Full Text PDF:/Users/tim/Zotero/storage/YIT8BYJJ/Sixt et al. - 2020 - When Explanations Lie Why Many Modified BP Attrib.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/D43FPAPU/af21d0c97db2e27e13572cbf59eb343d.html:text/html},
}

@misc{sixt_rendergan:_2017,
	title = {{RenderGAN}: {Generating} {Realistic} {Labeled} {Data}},
	shorttitle = {{RenderGAN}},
	url = {http://arxiv.org/abs/1611.01331},
	abstract = {Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.},
	urldate = {2020-09-19},
	author = {Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
	month = jan,
	year = {2017},
	note = {tex.ids= sixt\_rendergan:\_2016
arXiv: 1611.01331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, GAN},
	file = {[PDF] from arxiv.org:/Users/tim/Zotero/storage/U9IK2JRA/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv Fulltext PDF:/Users/tim/Zotero/storage/TG5Y5XMR/Sixt et al. - 2017 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv\:1611.01331 PDF:/Users/tim/Zotero/storage/W3IPQKCY/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv\:1611.01331 PDF:/Users/tim/Zotero/storage/6MNB3QP2/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/QVVAUA6F/1611.html:text/html;arXiv.org Snapshot:/Users/tim/Zotero/storage/LTB749VB/1611.html:text/html;arXiv.org Snapshot:/Users/tim/Zotero/storage/8NGCRGYK/1611.html:text/html;Snapshot:/Users/tim/Zotero/storage/62H28EUD/Sixt et al. - 2016 - RenderGAN Generating Realistic Labeled Data.html:text/html},
}

@article{boenisch_tracking_2018,
	title = {Tracking {All} {Members} of a {Honey} {Bee} {Colony} {Over} {Their} {Lifetime} {Using} {Learned} {Models} of {Correspondence}},
	volume = {5},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00035/full},
	doi = {10.3389/frobt.2018.00035},
	abstract = {Computational approaches to the analysis of collective behavior in social insects increasingly rely on motion paths as an intermediate data layer from which one can infer individual behaviors or social interactions. Honey bees are a popular model for learning and memory. Previous experience has been shown to affect and modulate future social interactions. So far, no lifetime history observations have been reported for all bees of a colony. In a previous work we introduced a recording setup customized to track up to 4000 marked bees over several weeks. Due to detection and decoding errors of the bee markers, linking the correct correspondences through time is non-trivial. In this contribution we present an in-depth description of the underlying multi-step algorithm which produces motion paths, and also improves the marker decoding accuracy significantly. The proposed solution employs two classifiers to predict the correspondence of two consecutive detections in the first step, and two tracklets in the second. We automatically tracked {\textasciitilde}2000 marked honey bees over 10 weeks with inexpensive recording hardware using markers without any error correction bits. We found that the proposed two-step tracking reduced incorrect ID decodings from initially {\textasciitilde}13 \% to around 2 \% post-tracking. Alongside this paper, we publish the first trajectory dataset for all bees in a colony, extracted from {\textasciitilde}3 million images covering three days. We invite researchers to join the collective scientific effort to investigate this intriguing animal system. All components of our system are open-source.},
	language = {English},
	urldate = {2019-05-27},
	journal = {Frontiers in Robotics and AI},
	author = {Boenisch, Franziska and Rosemann, Benjamin and Wild, Benjamin and Dormagen, David and Wario, Fernando and Landgraf, Tim},
	year = {2018},
	keywords = {Apis mellifera, peer-reviewed, honey bees, Lifetime History, social insects, tracking, trajectory},
	file = {Full Text PDF:/Users/tim/Zotero/storage/YSN3WRT6/Boenisch et al. - 2018 - Tracking All Members of a Honey Bee Colony Over Th.pdf:application/pdf;Full Text PDF:/Users/tim/Zotero/storage/NTWTRNTZ/Boenisch et al. - 2018 - Tracking All Members of a Honey Bee Colony Over Th.pdf:application/pdf},
}

@article{wild_social_2021,
	title = {Social networks predict the life and death of honey bees},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21212-5},
	doi = {10.1038/s41467-021-21212-5},
	abstract = {In complex societies, individuals’ roles are reflected by interactions with other conspecifics. Honey bees (Apis mellifera) generally change tasks as they age, but developmental trajectories of individuals can vary drastically due to physiological and environmental factors. We introduce a succinct descriptor of an individual’s social network that can be obtained without interfering with the colony. This ‘network age’ accurately predicts task allocation, survival, activity patterns, and future behavior. We analyze developmental trajectories of multiple cohorts of individuals in a natural setting and identify distinct developmental pathways and critical life changes. Our findings suggest a high stability in task allocation on an individual level. We show that our method is versatile and can extract different properties from social networks, opening up a broad range of future studies. Our approach highlights the relationship of social interactions and individual traits, and provides a scalable technique for understanding how complex social systems function.},
	language = {en},
	number = {1},
	urldate = {2021-02-17},
	journal = {Nature Communications},
	author = {Wild, Benjamin and Dormagen, David M. and Zachariae, Adrian and Smith, Michael L. and Traynor, Kirsten S. and Brockmann, Dirk and Couzin, Iain D. and Landgraf, Tim},
	month = feb,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1110},
	file = {Full Text PDF:/Users/tim/Zotero/storage/HLIDDXF4/Wild et al. - 2021 - Social networks predict the life and death of hone.pdf:application/pdf;Full Text PDF:/Users/tim/Zotero/storage/BLMH8GQX/Wild et al. - 2020 - Social networks predict the life and death of hone.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/U5GHA4FN/s41467-021-21212-5.html:text/html},
}

@phdthesis{landgraf_robobee:_2013,
	title = {{RoboBee}: {A} {Biomimetic} {Honeybee} {Robot} for the {Analysis} of the {Dance} {Communication} {System}},
	shorttitle = {{RoboBee}},
	url = {http://www.diss.fu-berlin.de/diss/receive/FUDISS_thesis_000000094818?lang=de},
	urldate = {2016-11-29},
	school = {Berlin, Freie Universität Berlin, 2013},
	author = {Landgraf, Tim},
	year = {2013},
}

@inproceedings{landgraf_blending_2014,
	title = {Blending in with the shoal: robotic fish swarms for investigating strategies of group formation in guppies},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer, Cham},
	author = {Landgraf, Tim and Nguyen, Hai and Schröer, Joseph and Szengel, Angelika and Clément, Romain JG and Bierbach, David and Krause, Jens},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {178--189},
}

@inproceedings{landgraf_interactive_2013,
	title = {Interactive robotic fish for the analysis of swarm behavior},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-38703-6_1},
	urldate = {2016-11-29},
	booktitle = {International {Conference} in {Swarm} {Intelligence}},
	booktitle = {International {Conference} in {Swarm} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Nguyen, Hai and Forgo, Stefan and Schneider, Jan and Schröer, Joseph and Krüger, Christoph and Matzke, Henrik and Clément, Romain O. and Krause, Jens and Rojas, Raúl},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {1--10},
	file = {[PDF] from researchgate.net:/Users/tim/Zotero/storage/M3D38ERD/Landgraf et al. - 2013 - Interactive robotic fish for the analysis of swarm.pdf:application/pdf},
}

@inproceedings{landgraf_multi-agent_2012,
	title = {A {Multi}-agent {Platform} for {Biomimetic} {Fish}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31525-1_44},
	urldate = {2016-11-29},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Akkad, Rami and Nguyen, Hai and Clément, Romain O. and Krause, Jens and Rojas, Raúl},
	year = {2012},
	keywords = {peer-reviewed},
	pages = {365--366},
}

@article{landgraf_design_2008,
	title = {Design and development of a robotic bee for the analysis of honeybee dance communication},
	volume = {5},
	url = {http://www.tandfonline.com/doi/abs/10.1080/11762320802617552},
	number = {3},
	urldate = {2016-11-29},
	journal = {Applied Bionics and Biomechanics},
	author = {Landgraf, T. and Moballegh, H. and Rojas, R.},
	year = {2008},
	keywords = {peer-reviewed},
	pages = {157--164},
}

@article{jin_walking_2014,
	title = {Walking bumblebees memorize panorama and local cues in a laboratory test of navigation},
	volume = {97},
	url = {http://www.sciencedirect.com/science/article/pii/S0003347214003273},
	urldate = {2016-11-29},
	journal = {Animal Behaviour},
	author = {Jin, Nanxiang and Landgraf, Tim and Klein, Simon and Menzel, Randolf},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {13--23},
	file = {[PDF] from researchgate.net:/Users/tim/Zotero/storage/JA7APBNT/Jin et al. - 2014 - Walking bumblebees memorize panorama and local cue.pdf:application/pdf},
}

@article{hussaini_sleep_2009,
	title = {Sleep deprivation affects extinction but not acquisition memory in honeybees},
	volume = {16},
	url = {http://learnmem.cshlp.org/content/16/11/698.short},
	number = {11},
	urldate = {2016-11-29},
	journal = {Learning \& memory},
	author = {Hussaini, Syed Abid and Bogusch, Lisa and Landgraf, Tim and Menzel, Randolf},
	year = {2009},
	keywords = {peer-reviewed},
	pages = {698--705},
}

@inproceedings{helgadottir_conditioned_2013,
	title = {Conditioned behavior in a robot controlled by a spiking neural network},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6696078},
	urldate = {2016-11-29},
	booktitle = {Neural {Engineering} ({NER}), 2013 6th {International} {IEEE}/{EMBS} {Conference} on},
	booktitle = {Neural {Engineering} ({NER}), 2013 6th {International} {IEEE}/{EMBS} {Conference} on},
	publisher = {IEEE},
	author = {Helgadóttir, Lovísa Irpa and Haenicke, Joachim and Landgraf, Tim and Rojas, Raul and Nawrot, Martin P.},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {891--894},
}

@article{bierbach_guppies_2020,
	title = {Guppies {Prefer} to {Follow} {Large} ({Robot}) {Leaders} {Irrespective} of {Own} {Size}},
	volume = {8},
	issn = {2296-4185},
	url = {https://www.frontiersin.org/articles/10.3389/fbioe.2020.00441/full},
	doi = {10.3389/fbioe.2020.00441},
	abstract = {Body size is often assumed to determine how successful an individual can lead others with larger individuals being better leaders than smaller ones. But even if larger individuals are more readily followed, body size often correlates with specific behavioral patterns and it is thus unclear whether larger individuals are more often followed than smaller ones because of their size or because they behave in a certain way. To control for behavioral differences among differentially-sized leaders, we used biomimetic robotic fish (Robofish) of different sizes. Live guppies (Poecilia reticulata) are known to interact with Robofish in a similar way as with live conspecifics. Consequently, Robofish may serve as a conspecific-like leader that provides standardized behaviors irrespective of its size. We asked whether larger Robofish leaders are preferentially followed and whether the preferences of followers depend on own body size or risk-taking behavior (‘boldness’). We found that live guppies followed larger Robofish leaders in closer proximity than smaller ones and this pattern was independent of the followers’ own body size as well as risk-taking behavior. Our study shows a ‘bigger is better’ pattern in leadership that is fully independent of behavioral differences among differentially-sized leaders, followers’ own size and risk-taking behavior.},
	language = {English},
	urldate = {2020-09-17},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Bierbach, David and Mönck, Hauke J. and Lukas, Juliane and Habedank, Marie and Romanczuk, Pawel and Landgraf, Tim and Krause, Jens},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {peer-reviewed, biomimetic robots, Body Size, Leadership, Poecilia reticulata, Robotic fish},
	file = {Full Text PDF:/Users/tim/Zotero/storage/I7PZ2KDL/Bierbach et al. - 2020 - Guppies Prefer to Follow Large (Robot) Leaders Irr.pdf:application/pdf},
}

@article{bierbach_insights_2018,
	title = {Insights into the {Social} {Behavior} of {Surface} and {Cave}-{Dwelling} {Fish} ({Poecilia} mexicana) in {Light} and {Darkness} through the {Use} of a {Biomimetic} {Robot}},
	volume = {5},
	issn = {2296-9144},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2018.00003/full},
	doi = {10.3389/frobt.2018.00003},
	urldate = {2018-02-13},
	journal = {Frontiers in Robotics and AI},
	author = {Bierbach, David and Lukas, Juliane and Bergmann, Anja and Elsner, Kristiane and Höhne, Leander and Weber, Christiane and Weimar, Nils and Arias-Rodriguez, Lenin and Mönck, Hauke J. and Nguyen, Hai and Romanczuk, Pawel and Landgraf, Tim and Krause, Jens},
	month = feb,
	year = {2018},
	keywords = {peer-reviewed},
}

@article{wario_automatic_2015,
	title = {Automatic methods for long-term tracking and the detection and decoding of communication dances in honeybees},
	volume = {3},
	issn = {2296-701X},
	doi = {10.3389/fevo.2015.00103},
	abstract = {The honeybee waggle dance communication system is an intriguing example of abstract animal communication and has been investigated thoroughly throughout the last seven decades. Typically, observables such as waggle durations or body angles are extracted manually either directly from the observation hive or from video recordings to quantify properties of the dance and related behaviors. In recent years, biology has proﬁted from automation, improving measurement precision, removing human bias, and accelerating data collection. We have developed technologies to track all individuals of a honeybee colony and to detect and decode communication dances automatically. In strong contrast to conventional approaches that focus on a small subset of the hive life, whether this regards time, space, or animal identity, our more inclusive system will help the understanding of the dance comprehensively in its spatial, temporal, and social context. In this contribution, we present full speciﬁcations of the recording setup and the software for automatic recognition of individually tagged bees and the decoding of dances. We discuss potential research directions that may beneﬁt from the proposed automation. Lastly, to exemplify the power of the methodology, we show experimental data and respective analyses from a continuous, experimental recording of 9 weeks duration.},
	language = {en},
	urldate = {2019-05-27},
	journal = {Frontiers in Ecology and Evolution},
	author = {Wario, Fernando and Wild, Benjamin and Couvillon, Margaret J. and Rojas, Raúl and Landgraf, Tim},
	month = sep,
	year = {2015},
	keywords = {waggle dance, animal behavior, peer-reviewed, honeybee, animal tracking},
	file = {Full Text PDF:/Users/tim/Zotero/storage/84S33XGB/Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:application/pdf;Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:/Users/tim/Zotero/storage/5LIJ56TL/Wario et al. - 2015 - Automatic methods for long-term tracking and the d.pdf:application/pdf},
}

@article{bierbach_using_2018,
	title = {Using a robotic fish to investigate individual differences in social responsiveness in the guppy},
	volume = {5},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.181026},
	doi = {10.1098/rsos.181026},
	abstract = {Responding towards the actions of others is one of the most important behavioural traits whenever animals of the same species interact. Mutual influences among interacting individuals may modulate the social responsiveness seen and thus make it often difficult to study the level and individual variation in responsiveness. Here, open-loop biomimetic robots that provide standardized, non-interactive social cues can be a useful tool. These robots are not affected by the live animal's actions but are assumed to still represent valuable and biologically relevant social cues. As this assumption is crucial for the use of biomimetic robots in behavioural studies, we hypothesized (i) that meaningful social interactions can be assumed if live animals maintain individual differences in responsiveness when interacting with both a biomimetic robot and a live partner. Furthermore, to study the level of individual variation in social responsiveness, we hypothesized (ii) that individual differences should be maintained over the course of multiple tests with the robot. We investigated the response of live guppies (Poecilia reticulata) when allowed to interact either with a biomimetic open-loop-controlled fish robot—‘Robofish’—or with a live companion. Furthermore, we investigated the responses of live guppies when tested three times with Robofish. We found that responses of live guppies towards Robofish were weaker compared with those of a live companion, most likely as a result of the non-interactive open-loop behaviour of Robofish. Guppies, however, were consistent in their individual responses between a live companion and Robofish, and similar individual differences in response towards Robofish were maintained over repeated testing even though habituation to the test environment was detectable. Biomimetic robots like Robofish are therefore a useful tool for the study of social responsiveness in guppies and possibly other small fish species.},
	number = {8},
	urldate = {2019-02-11},
	journal = {Royal Society Open Science},
	author = {Bierbach, David and Landgraf, Tim and Romanczuk, Pawel and Lukas, Juliane and Nguyen, Hai and Wolf, Max and Krause, Jens},
	year = {2018},
	keywords = {peer-reviewed},
	pages = {181026},
	file = {Full Text PDF:/Users/tim/Zotero/storage/7HVL42GR/Bierbach David et al. - Using a robotic fish to investigate individual dif.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/RUVXW6RG/rsos.html:text/html},
}

@incollection{landgraf_kunstliche_2017,
	title = {Künstliche {Mini}-{Gehirne} für {Roboter}},
	booktitle = {Planen und {Handeln}},
	publisher = {Springer Spektrum, Wiesbaden},
	author = {Landgraf, Tim and Nawrot, Martin},
	year = {2017},
	pages = {135--150},
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} {Unsupervised} {Neural} {Coding} of {Nightingale} {Songs} {Using} {Deep} {Autoencoders}},
	url = {https://www.frontiersin.org/10.3389/conf.fncom.2011.53.00078/event_abstract},
	abstract = {In this work we tested different deep autoencoder networks for the unsupervised feature extraction of nightingale song spectra. Nightingales are versatile singers. Some individuals can sing up to 600 different songs. Biologists analyzing nightingale communication have to classify audio recordings by comparing hundreds of unknown songs to a database. This work is done mostly manually by matching spectrograms visually. An automatic or semi-automatic method for song classification would speed up this tedious process. We have utilized a recently published learning method to train multi-layered (‘deep’) artificial neural networks to reduce the dimensionality of – and find correlations within the spectrogram data in an unsupervised manner. We propose several preprocessing steps and network topologies to find low dimensional representations of nightingale songs. First, the audio data is filtered with a band pass to reduce low-frequency noise, e.g. of nearby cars. Then, we normalize the volume and down-scale the spectrograms to 256 x 400 points. This matrix is used as the input layer to the network. The next layer extracts visual features like edges and corner points. Each neuron in that layer serves as a feature detector and shares its incoming weights with different ‘receptive fields’ in the input layer and thus establishes repetition- and shift invariance. The output of this layer will be fed to the next three layers that serve for the dimensionality reduction and are trained as proposed by Hinton (2006). The weights of the network are tuned by comparing the input of the network to its reconstruction: By feeding an input song to the network, a specific code can be read from the last, 16-dimensional, code layer. By projecting back the activity of this layer to the receptive field, using the same weights, it is possible to reconstruct its original excitation; a procedure we use also to measure the quality of the code. Once the training is complete it is possible to classify unknown songs using the low dimensional code with an additional classification layer or other standard classification methods.},
	urldate = {2020-04-13},
}

@misc{paffhausen_neural_2019,
	address = {Göttingen},
	title = {Neural correlates of mushroom body output neurons measured during flight of a harnessed honey bee on a quad copter},
	abstract = {Honey bee navigation is an actively investigated field but the knowledge about the neural correlates of goal directed long distance navigation remains mostly unknown. We recorded single neuron activity during flight in a natural environment. Bees were trained to a feeder 400 m from the hive. Spiking activity of high order interneurons (mushroom body extrinsic neurons of the A3 cluster) were recorded with extracellular electrodes at the alpha exit. The bees were attached to a quad copter together with the necessary amplifiers and data storing devices. The copter flew along the path the bees had taken during training to the feeder. Additional flight paths were flown at natural speed and height. The spike rates of the recorded neurons were analyzed with respect to the corresponding flight tracks. Preliminary analyses of the data showed a strong and repeated spike rate change whenever the copter turned in tight bends. Straight flights resulted in partially repeated spike rate changes along similar stretches of the flight path but with much higher variance. Further analyses are on the way. The next experiments will include other flight paths focusing on the question whether spiking activity can be related to object identification and localization, properties that are considered to be involved in mushroom body function.},
	author = {Paffhausen, Benjamin and Petrasch, Julian and Wild, Benjamin and Fuchs, Inga and Drexler, Helmut and Kuriatnyk, Oleksandra and Meurers, Thierry and Landgraf, Tim and Menzel, Randolf},
	month = jun,
	year = {2019},
	file = {Snapshot:/Users/tim/Zotero/storage/VXQP5I7L/331981513_Neural_correlates_of_mushroom_body_output_neurons_measured_during_flight_of_a_harness.html:text/html},
}

@article{worm_evidence_2018,
	title = {Evidence for mutual allocation of social attention through interactive signaling in a mormyrid weakly electric fish},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/26/6852},
	doi = {10.1073/pnas.1801283115},
	abstract = {Mormyrid weakly electric fish produce electric organ discharges (EODs) for active electrolocation and electrocommunication. These pulses are emitted with variable interdischarge intervals (IDIs) resulting in temporal discharge patterns and interactive signaling episodes with nearby conspecifics. However, unequivocal assignment of interactive signaling to a specific behavioral context has proven to be challenging. Using an ethorobotical approach, we confronted single individuals of weakly electric Mormyrus rume proboscirostris with a mobile fish robot capable of interacting both physically, on arbitrary trajectories, as well as electrically, by generating echo responses through playback of species-specific EODs, thus synchronizing signals with the fish. Interactive signaling by the fish was more pronounced in response to a dynamic echo playback generated by the robot than in response to playback of static random IDI sequences. Such synchronizations were particularly strong at a distance corresponding to the outer limit of active electrolocation, and when fish oriented toward the fish replica. We therefore argue that interactive signaling through echoing of a conspecific’s EODs provides a simple mechanism by which weakly electric fish can specifically address nearby individuals during electrocommunication. Echoing may thus enable mormyrids to mutually allocate social attention and constitute a foundation for complex social behavior and relatively advanced cognitive abilities in a basal vertebrate lineage.},
	language = {en},
	number = {26},
	urldate = {2019-10-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Worm, Martin and Landgraf, Tim and Prume, Julia and Nguyen, Hai and Kirschbaum, Frank and Emde, Gerhard von der},
	month = jun,
	year = {2018},
	keywords = {peer-reviewed, electrocommunication, ethorobotics, interactive signaling, social attention},
	pages = {6852--6857},
	file = {Full Text PDF:/Users/tim/Zotero/storage/TIHK2EP2/Worm et al. - 2018 - Evidence for mutual allocation of social attention.pdf:application/pdf},
}

@article{landgraf_robofish:_2016,
	title = {{RoboFish}: increased acceptance of interactive robotic fish with realistic eyes and natural motion patterns by live {Trinidadian} guppies},
	volume = {11},
	number = {1},
	journal = {Bioinspiration \& Biomimetics},
	author = {Landgraf, Tim and Bierbach, David and Nguyen, Hai and Muggelberg, Nadine and Romanczuk, Pawel and Krause, Jens},
	year = {2016},
	keywords = {peer-reviewed},
	pages = {015001},
	file = {[PDF] from researchgate.net:/Users/tim/Zotero/storage/7CP7I5V2/Landgraf et al. - 2016 - RoboFish increased acceptance of interactive robo.pdf:application/pdf},
}

@article{lam_dancing_2017,
	title = {Dancing attraction: followers of honey bee tremble and waggle dances exhibit similar behaviors},
	journal = {Biology open},
	author = {Lam, Calvin and Li, Yanlei and Landgraf, Tim and Nieh, James},
	year = {2017},
	keywords = {peer-reviewed},
	pages = {bio--025445},
}

@inproceedings{meyer_digital_2011,
	title = {A digital receptor neuron connecting remote sensor hardware to spiking neural networks},
	booktitle = {{BC11} : {Computational} {Neuroscience} \& {Neurotechnology} {Bernstein} {Conference} \& {Neurex} {Annual} {Meeting} 2011, {Freiburg}, {Germany}, 4 {Oct} - 6 {Oct}, 2011.},
	booktitle = {{BC11} : {Computational} {Neuroscience} \& {Neurotechnology} {Bernstein} {Conference} \& {Neurex} {Annual} {Meeting} 2011, {Freiburg}, {Germany}, 4 {Oct} - 6 {Oct}, 2011.},
	publisher = {Frontiers Neuroscience},
	author = {Meyer, Jan and Haenicke, Joachim and Landgraf, Tim and Schmuker, Michael and Rojas, Raúl and Nawrot, Martin},
	year = {2011},
}

@inproceedings{landgraf_blending_2011,
	title = {Blending into the {Hive}: {A} {Novel} {Biomimetic} {Honeybee} {Robot} for the {Analysis} of the {Dance} {Communication} {System}.},
	booktitle = {International {Workshop} on {Bio}-{Inspired} {Robots}, {Nantes} {April} 6-8},
	booktitle = {International {Workshop} on {Bio}-{Inspired} {Robots}, {Nantes} {April} 6-8},
	publisher = {Ecole des Mines, IRCCYN LAB},
	author = {Landgraf, Tim},
	year = {2011},
	keywords = {peer-reviewed},
}

@inproceedings{helgadottir_robotic_2012,
	title = {A {Robotic} {Platform} for {Spiking} {Neural} {Control} {Architectures}},
	booktitle = {Bernstein {Conference} 2012, {Munich}, {Germany}, 12 {Sep} - 14 {Sep}, 2012.},
	booktitle = {Bernstein {Conference} 2012, {Munich}, {Germany}, 12 {Sep} - 14 {Sep}, 2012.},
	publisher = {Frontiers in Computational Neuroscience},
	author = {Helgadottir, Lovisa Irpa and Haenicke, Joachim and Landgraf, Tim and Nawrot, Martin Paul},
	year = {2012},
	pages = {154},
}

@inproceedings{worm_electro-communicating_2014,
	title = {Electro-communicating dummy fish initiate group behavior in the weakly electric fish {Mormyrus} rume},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer, Cham},
	author = {Worm, Martin and Landgraf, Tim and Nguyen, Hai and von der Emde, Gerhard},
	year = {2014},
	keywords = {peer-reviewed},
	pages = {446--448},
}

@inproceedings{landgraf_neurocopter:_2013,
	title = {{NeuroCopter}: neuromorphic computation of {6D} ego-motion of a quadcopter},
	shorttitle = {{NeuroCopter}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-39802-5_13},
	urldate = {2016-11-29},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Landgraf, Tim and Wild, Benjamin and Ludwig, Tobias and Nowak, Philipp and Helgadottir, Lovisa and Daumenlang, Benjamin and Breinlinger, Philipp and Nawrot, Martin and Rojas, Raúl},
	year = {2013},
	keywords = {peer-reviewed},
	pages = {143--153},
}

@article{landgraf_analysis_2011,
	title = {Analysis of the {Waggle} {Dance} {Motion} of {Honeybees} for the {Design} of a {Biomimetic} {Honeybee} {Robot}},
	volume = {6},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0021354},
	doi = {10.1371/journal.pone.0021354},
	language = {en},
	number = {8},
	urldate = {2017-10-23},
	journal = {PLoS ONE},
	author = {Landgraf, Tim and Rojas, Raúl and Nguyen, Hai and Kriegel, Fabian and Stettin, Katja},
	editor = {Krapp, Holger G.},
	month = aug,
	year = {2011},
	keywords = {peer-reviewed},
	pages = {e21354},
	file = {journal.pone.0021354.PDF:/Users/tim/Zotero/storage/74ESK9DB/journal.pone.0021354.PDF:application/pdf},
}

@inproceedings{landgraf_imitation_2012,
	title = {Imitation of the honeybee dance communication system by means of a biomimetic robot},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-31525-1_12},
	urldate = {2016-12-20},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	booktitle = {Conference on {Biomimetic} and {Biohybrid} {Systems}},
	publisher = {Springer},
	author = {Landgraf, Tim and Oertel, Michael and Kirbach, Andreas and Menzel, Randolf and Rojas, Raúl},
	year = {2012},
	keywords = {peer-reviewed},
	pages = {132--143},
	file = {Landgraf et al._2012_Imitation of the honeybee dance communication system by means of a biomimetic robot.pdf:/Users/tim/Zotero/storage/X5N679X3/Landgraf et al._2012_Imitation of the honeybee dance communication system by means of a biomimetic robot.pdf:application/pdf},
}

@techreport{landgraf_tracking_2007,
	title = {Tracking honey bee dances from sparse optical flow fields},
	copyright = {http://www.fu-berlin.de/sites/refubium/rechtliches/Nutzungsbedingungen},
	url = {https://refubium.fu-berlin.de/handle/fub188/19039},
	language = {eng},
	urldate = {2021-09-13},
	author = {Landgraf, Tim and Rojas, Raúl},
	year = {2007},
	note = {Accepted: 2018-06-08T07:56:28Z},
	file = {Full Text PDF:/Users/tim/Zotero/storage/4QF4QYNY/Landgraf and Rojas - 2007 - Tracking honey bee dances from sparse optical flow.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/PFKH9672/19039.html:text/html},
}

@inproceedings{bierbach_biomimetic_2021,
	title = {Biomimetic robots promote the {3Rs} {Principle} in animal testing},
	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00375/102921/Biomimetic-robots-promote-the-3Rs-Principle-in},
	doi = {10.1162/isal_a_00375},
	language = {en},
	urldate = {2021-09-13},
	booktitle = {{ALIFE} 2021: {The} 2021 {Conference} on {Artificial} {Life}},
	publisher = {MIT Press},
	author = {Bierbach, David and Francisco, Fritz and Lukas, Juliane and Landgraf, Tim and Maxeiner, Moritz and Romanczuk, Pawel and Musiolek, Lea and Hafner, Verena V. and Krause, Jens},
	month = jul,
	year = {2021},
	file = {Full Text PDF:/Users/tim/Zotero/storage/IFN9HUVQ/Bierbach et al. - 2021 - Biomimetic robots promote the 3Rs Principle in ani.pdf:application/pdf},
}

@inproceedings{ilgun_bio-hybrid_2021,
	title = {Bio-{Hybrid} {Systems} for {Ecosystem} {Level} {Effects}},
	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00396/102910/Bio-Hybrid-Systems-for-Ecosystem-Level-Effects},
	doi = {10.1162/isal_a_00396},
	language = {en},
	urldate = {2021-09-13},
	booktitle = {{ALIFE} 2021: {The} 2021 {Conference} on {Artificial} {Life}},
	publisher = {MIT Press},
	author = {Ilgün, Asya and Angelov, Kostadin and Stefanec, Martin and Schönwetter-Fuchs, Sarah and Stokanic, Valerin and Vollmann, Jutta and Hofstadler, Daniel N. and Kärcher, Martin H. and Mellmann, Heinrich and Taliaronak, Volha and Kviesis, Armands and Komasilovs, Vitalijs and Becher, Matthias A. and Szopek, Martina and Dormagen, David M. and Barmak, Rafael and Bairaktarov, Erol and Broisin, Matthieu and Thenius, Ronald and Mills, Rob and Nicolis, Stamatios C. and Campo, Alexandre and Zacepins, Aleksejs and Petrov, Sergey and Deneubourg, Jean-Louis and Mondada, Francesco and Landgraf, Tim and Hafner, Verena V. and Schmickl, Thomas},
	month = jul,
	year = {2021},
	file = {Full Text PDF:/Users/tim/Zotero/storage/SYKQZQ2Q/Ilgün et al. - 2021 - Bio-Hybrid Systems for Ecosystem Level Effects.pdf:application/pdf},
}

@article{worm_electric_2021,
	title = {Electric signal synchronization as a behavioural strategy to generate social attention in small groups of mormyrid weakly electric fish and a mobile fish robot},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-021-00892-8},
	doi = {10.1007/s00422-021-00892-8},
	abstract = {African weakly electric fish communicate at night by constantly emitting and perceiving brief electrical signals (electric organ discharges, EOD) at variable inter-discharge intervals (IDI). While the waveform of single EODs contains information about the sender’s identity, the variable IDI patterns convey information about its current motivational and behavioural state. Pairs of fish can synchronize their EODs to each other via echo responses, and we have previously formulated a ‘social attention hypothesis’ stating that fish use echo responses to address specific individuals and establish brief dyadic communication frameworks within a group. Here, we employed a mobile fish robot to investigate the behaviour of small groups of up to four Mormyrus rume and characterized the social situations during which synchronizations occurred. An EOD-emitting robot reliably evoked social following behaviour, which was strongest in smaller groups and declined with increasing group size. We did not find significant differences in motor behaviour of M. rume with either an interactive playback (echo response) or a random control playback by the robot. Still, the robot reliably elicited mutual synchronizations with other fish. Synchronizations mostly occurred during relatively close social interactions, usually when the fish that initiated synchronization approached either the robot or another fish from a distance. The results support our social attention hypothesis and suggest that electric signal synchronization might facilitate the exchange of social information during a wide range of social behaviours from aggressive territorial displays to shoaling and even cooperative hunting in some mormyrids.},
	language = {en},
	urldate = {2021-09-13},
	journal = {Biological Cybernetics},
	author = {Worm, Martin and Landgraf, Tim and von der Emde, Gerhard},
	month = aug,
	year = {2021},
	file = {Springer Full Text PDF:/Users/tim/Zotero/storage/U3SMTJBR/Worm et al. - 2021 - Electric signal synchronization as a behavioural s.pdf:application/pdf},
}

@article{wario_automatic_2017,
	title = {Automatic detection and decoding of honey bee waggle dances},
	volume = {12},
	issn = {1932-6203},
	url = {http://arxiv.org/abs/1708.06590},
	doi = {10.1371/journal.pone.0188626},
	abstract = {The waggle dance is one of the most popular examples of animal communication. Forager bees direct their nestmates to profitable resources via a complex motor display. Essentially, the dance encodes the polar coordinates to the resource in the field. Unemployed foragers follow the dancer's movements and then search for the advertised spots in the field. Throughout the last decades, biologists have employed different techniques to measure key characteristics of the waggle dance and decode the information it conveys. Early techniques involved the use of protractors and stopwatches to measure the dance orientation and duration directly from the observation hive. Recent approaches employ digital video recordings and manual measurements on screen. However, manual approaches are very time-consuming. Most studies, therefore, regard only small numbers of animals in short periods of time. We have developed a system capable of automatically detecting, decoding and mapping communication dances in real-time. In this paper, we describe our recording setup, the image processing steps performed for dance detection and decoding and an algorithm to map dances to the field. The proposed system performs with a detection accuracy of 90.07{\textbackslash}\%. The decoded waggle orientation has an average error of -2.92\{{\textbackslash}deg\} (\${\textbackslash}pm\$ 7.37\{{\textbackslash}deg\} ), well within the range of human error. To evaluate and exemplify the system's performance, a group of bees was trained to an artificial feeder, and all dances in the colony were automatically detected, decoded and mapped. The system presented here is the first of this kind made publicly available, including source code and hardware specifications. We hope this will foster quantitative analyses of the honey bee waggle dance.},
	number = {12},
	urldate = {2020-09-19},
	journal = {PLOS ONE},
	author = {Wario, Fernando and Wild, Benjamin and Rojas, Raúl and Landgraf, Tim},
	month = dec,
	year = {2017},
	note = {arXiv: 1708.06590},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, peer-reviewed, Quantitative Biology - Quantitative Methods},
	pages = {e0188626},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/DQ8SHPPE/Wario et al. - 2017 - Automatic detection and decoding of honey bee wagg.pdf:application/pdf;arXiv\:1708.06590 PDF:/Users/tim/Zotero/storage/ZAXSM337/Wario et al. - 2017 - Automatic detection and decoding of honey bee wagg.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/2C2K56IR/1708.html:text/html;arXiv.org Snapshot:/Users/tim/Zotero/storage/LEA5ML6P/1708.html:text/html},
}

@article{sixt_rendergan:_2018,
	title = {{RenderGAN}: {Generating} {Realistic} {Labeled} {Data}},
	volume = {5},
	issn = {2296-9144},
	shorttitle = {{RenderGAN}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2018.00066/full},
	doi = {10.3389/frobt.2018.00066},
	abstract = {Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.},
	language = {English},
	urldate = {2021-03-02},
	journal = {Frontiers in Robotics and AI},
	author = {Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
	year = {2018},
	note = {publisher: Frontiers},
	keywords = {peer-reviewed, deep learning, social insects, unsupervised learning, Generative Adversarial Networks, markers},
	file = {Full Text PDF:/Users/tim/Zotero/storage/GXKHENIN/Sixt et al. - 2018 - RenderGAN Generating Realistic Labeled Data.pdf:application/pdf},
}

@article{muller_neural_2018,
	title = {A neural network model for familiarity and context learning during honeybee foraging flights},
	volume = {112},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/s00422-017-0732-z},
	doi = {10.1007/s00422-017-0732-z},
	language = {en},
	number = {1-2},
	urldate = {2018-12-13},
	journal = {Biological Cybernetics},
	author = {Müller, Jurek and Nawrot, Martin and Menzel, Randolf and Landgraf, Tim},
	month = apr,
	year = {2018},
	keywords = {peer-reviewed},
	pages = {113--126},
	file = {Müller et al. - 2018 - A neural network model for familiarity and context.pdf:/Users/tim/Zotero/storage/8C4NMSVE/Müller et al. - 2018 - A neural network model for familiarity and context.pdf:application/pdf},
}

@inproceedings{landgraf_biomimetic_2010,
	title = {A biomimetic honeybee robot for the analysis of the honeybee dance communication system},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2010 {IEEE}/{RSJ} {International} {Conference} on},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2010 {IEEE}/{RSJ} {International} {Conference} on},
	publisher = {IEEE},
	author = {Landgraf, Tim and Oertel, Michael and Rhiel, Daniel and Rojas, Raúl},
	year = {2010},
	keywords = {peer-reviewed, Im Text, biomimetics, mobile robots, computer vision, Cameras, Robot vision systems, Robot kinematics, biomimetic honeybee robot, hardware design, honeybee dance communication system, imminent collisions, intelligent robots, Oscillators, software design, Synchronous motors},
	pages = {3097--3102},
}

@article{sixt_interpretability_2020,
	title = {Interpretability {Through} {Invertibility}: {A} {Deep} {Convolutional} {Network} {With} {Ideal} {Counterfactuals} {And} {Isosurfaces}},
	shorttitle = {Interpretability {Through} {Invertibility}},
	url = {https://openreview.net/forum?id=8YFhXYe1Ps},
	abstract = {Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the...},
	language = {en},
	urldate = {2021-07-22},
	author = {Sixt, Leon and Schuessler, Martin and Weiß, Philipp and Landgraf, Tim},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/tim/Zotero/storage/ELXMRHJP/Sixt et al. - 2020 - Interpretability Through Invertibility A Deep Con.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/D6LQBAIN/forum.html:text/html},
}

@misc{wild_individuality_2020,
	title = {Individuality in the hive - {Learning} to embed lifetime social behaviour of honey bees},
	url = {https://openreview.net/forum?id=2LBhynkS2SC},
	abstract = {Honey bees are a popular model for complex social systems, in which global behavior emerges from the actions and interactions of thousands of individuals. While the average life of a bee is...},
	language = {en},
	urldate = {2021-07-22},
	author = {Wild, Benjamin and Dormagen, David and Smith, Michael L. and Landgraf, Tim},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/tim/Zotero/storage/EHQ4PJ4C/Wild et al. - 2020 - Individuality in the hive - Learning to embed life.pdf:application/pdf;Snapshot:/Users/tim/Zotero/storage/9MQ4ETMU/forum.html:text/html},
}

@article{paffhausen_flying_2021,
	title = {A flying platform to investigate neuronal correlates of navigation in the honey bee ({Apis} mellifera)},
	volume = {15},
	issn = {1662-5153},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2021.690571/abstract},
	doi = {10.3389/fnbeh.2021.690571},
	abstract = {Navigating animals combine multiple perceptual faculties, learn during exploration, retrieve multi-facetted memory contents, and exhibit goal-directedness as an expression of their current needs and motivations. Navigation in insects has been linked to a variety of underlying strategies such as path integration, view familiarity, visual beaconing and goal-directed orientation with respect to previously learned ground structures. Most works, however, study navigation either from a field perspective, analyzing purely behavioral observations, or combine computational models with neurophysiological evidence obtained from lab experiments. The honey bee (Apis mellifera) has long been a popular model in the search for neural correlates of complex behaviors and exhibits extraordinary navigational capabilities. However, the neural basis for bee navigation has not yet been explored under natural conditions. Here, we propose a novel methodology to record from the brain of a copter-mounted honey bee. This way, the animal experiences natural multimodal sensory inputs in a natural environment that is familiar to her. We have developed a miniaturized electrophysiology recording system which is able to record spikes in the presence of time-varying electric noise from the copter’s motors and rotors, and devised an experimental procedure to record from mushroom body extrinsic neurons (MBENs). We analyze the resulting electrophysiological data combined with a reconstruction of the animal’s visual perception and find that the neural activity of MBENs is linked to sharp turns, possibly related to the relative motion of visual features. This method is a significant technological step towards recording brain activity of navigating honey bees under natural conditions. By providing all system specifications in an online repository, we hope to close a methodological gap and stimulate further research informing future computational models of insect navigation.},
	language = {English},
	urldate = {2021-07-13},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Paffhausen, Benjamin Hans and Petrasch, Julian and Wild, Benjamin and Meurers, Thierry and Schülke, Tobias and Polster, Johannes and Fuchs, Inga and Drexler, Helmut and Kuriatnyk, Oleksandra and Menzel, Randolf and Landgraf, Tim},
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {mushroom body, navigation, Electrophysiology, Honeybee (Apis mellifera L), naturalistic condition, Neuroethology, Quad copter},
}

@article{menzel_guidance_2019,
	title = {Guidance of {Navigating} {Honeybees} by {Learned} {Elongated} {Ground} {Structures}},
	volume = {12},
	issn = {1662-5153},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2018.00322/full},
	doi = {10.3389/fnbeh.2018.00322},
	abstract = {Elongated landscape features like forest edges, rivers, roads or boundaries of fields are particularly salient landmarks for navigating animals. Here we ask how honeybees learn such structures and how they are used during their homing flights after being released at an unexpected location (catch-and-release paradigm). The experiments were performed in two landscapes that differed with respect to their overall structure: a rather feature-less landscape, and one rich in close and far distant landmarks. We tested three different forms of learning: learning during orientation flights, learning during training to a feeding site, and learning during homing flights after release at an unexpected site within the explored area. We found that bees use elongated ground structures, e.g. a field boundary separating two pastures close to the hive (experiment 1), an irrigation channel (experiment 2), a hedgerow along which the bees were trained (experiment 3), a gravel road close to the hive and the feeder (experiment 4), a path along an irrigation channel with its vegetation close to the feeder (experiment 5) and a gravel road along which bees performed their homing flights (experiment 6). Discrimination and generalization between the learned linear landmarks and similar ones in the test area depend on their object properties (irrigation channel, gravel road, hedgerow) and their compass orientation. We conclude that elongated ground structures are embedded into multiple landscape features indicating that memory of these linear structures is one component of bee navigation. Elongated structures interact and compete with other references. Object identification is an important part of this process. The objects are characterized not only by their appearance but also by their alignment in the compass. Their salience is highest if both components are close to what had been learned. High similarity in appearance can compensate for (partial) compass misalignment, and vice versa.},
	language = {English},
	urldate = {2021-06-05},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Menzel, Randolf and Tison, Lea and Fischer-Nakai, Johannes and Cheeseman, James and Balbuena, Maria Sol and Chen, Xiuxian and Landgraf, Tim and Petrasch, Julian and Polster, Johannes and Greggers, Uwe},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {object recognition, navigation, Compass alignment, ground structures, guiding landmarks, sun compass},
	file = {Full Text PDF:/Users/tim/Zotero/storage/7YTHDVUA/Menzel et al. - 2019 - Guidance of Navigating Honeybees by Learned Elonga.pdf:application/pdf},
}

@inproceedings{musiolek_robofish_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Robofish as {Social} {Partner} for {Live} {Guppies}},
	isbn = {978-3-030-64313-3},
	doi = {10.1007/978-3-030-64313-3_26},
	abstract = {Biomimetic robots that are accepted as social partners by animals may help to gain insights into animals’ social interaction skills. Here, we present an experiment using the biomimetic Robofish which resembles live guppies (Poecilia reticulata) - a small tropical freshwater fish. Guppy females were given the opportunity to interact with different open-loop controlled Robofish replicas. We show that guppies interacting with a lifelike Robofish replica scored higher on social interaction variables than did those faced with a simple white cuboid performing the same movements, although this effect weakened with time. Our study exemplifies the use of Robofish as a research tool, providing highly standardized social cues for the study of fish social skills such as imitation and following.},
	language = {en},
	booktitle = {Biomimetic and {Biohybrid} {Systems}},
	booktitle = {Biomimetic and {Biohybrid} {Systems}},
	publisher = {Springer International Publishing},
	author = {Musiolek, Lea and Hafner, Verena V. and Krause, Jens and Landgraf, Tim and Bierbach, David},
	editor = {Vouloutsi, Vasiliki and Mura, Anna and Tauber, Falk and Speck, Thomas and Prescott, Tony J. and Verschure, Paul F. M. J.},
	year = {2020},
	keywords = {Fish, Biorobotics, Social interaction.},
	pages = {270--274},
	file = {Springer Full Text PDF:/Users/tim/Zotero/storage/HSJYGGCS/Musiolek et al. - 2020 - Robofish as Social Partner for Live Guppies.pdf:application/pdf},
}

@inproceedings{wario_motion_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Motion {Dynamics} of {Foragers} in {Honey} {Bee} {Colonies}},
	isbn = {978-3-030-60376-2},
	doi = {10.1007/978-3-030-60376-2_16},
	abstract = {Information transfer among foragers is key for efficient allocation of work and adaptive responses within a honey bee colony. For information to spread quickly, foragers trying to recruit nestmates via the waggle dance (dancers) must reach as many other non-dancing foragers (followers) as possible. Forager bees may have different drives that influence their motion patterns. For instance, dancer bees need to widely cover the dance floor to recruit nestmates, the more broadly, the higher the food source profitability. Followers may instead move more erratically in the hope of meeting a dance. Overall, a good mixing of individuals is necessary to have flexibility at the level of the colony behavior and optimally respond to changing environmental conditions. We aim to determine the motion pattern that precedes communication events, exploiting a data-driven computational model. To this end, real observation data are used to define nest features such as the dance floor location, shape and size, as well as the foragers’ population size and density distribution. All these characteristics highly correlate with the bees walking pattern and determine the efficiency of information transfer among bees. A simulation environment is deployed to test different mobility patterns and evaluate the adherence with available real-world data. Additionally, we determine under what conditions information transfer is most efficient and effective. Owing to the simulation results, we identify the most plausible mobility pattern to represent the available observations.},
	language = {en},
	booktitle = {Swarm {Intelligence}},
	booktitle = {Swarm {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Wario, Fernando and Wild, Benjamin and Dormagen, David and Landgraf, Tim and Trianni, Vito},
	editor = {Dorigo, Marco and Stützle, Thomas and Blesa, Maria J. and Blum, Christian and Hamann, Heiko and Heinrich, Mary Katherine and Strobel, Volker},
	year = {2020},
	pages = {203--215},
	file = {Springer Full Text PDF:/Users/tim/Zotero/storage/4IRTUSBJ/Wario et al. - 2020 - Motion Dynamics of Foragers in Honey Bee Colonies.pdf:application/pdf},
}

@article{lukas_consistent_2021,
	title = {Consistent {Behavioral} {Syndrome} {Across} {Seasons} in an {Invasive} {Freshwater} {Fish}},
	volume = {8},
	issn = {2296-701X},
	url = {https://www.frontiersin.org/articles/10.3389/fevo.2020.583670/full?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound},
	doi = {10.3389/fevo.2020.583670},
	abstract = {Understanding the linkage between behavioral types and dispersal tendency has become a pressing issue in light of global change and biological invasions. Here, we explore whether dispersing individuals exhibit behavioral types that differ from those remaining in the source population. We investigated a feral population of guppies (Poecilia reticulata) that undergoes a yearly range shift cycle. Guppies are among the most widespread invasive species in the world, but in temperate regions these tropical fish can only survive in winter-warm freshwaters. Established in a thermally-altered stream in Germany, guppies are confined to a warm-water influx in winter, but can spread to peripheral parts as these become thermally accessible. We sampled fish from the source population and a winter-abandoned site in March, June and August. Fish were tested for boldness, sociability and activity involving open-field tests including interactions with a robotic social partner. Guppies differed consistently among each other in all three traits. Behavioral trait expression in the source population differed across seasons, however, we could not detect differences between source and downstream populations. Instead, all sampled populations exhibited a remarkably stable behavioral syndrome between boldness and activity despite strong seasonal changes in water temperature and associated environmental factors. We conclude that random drift (opposed to personality-biased dispersal) is a more likely dispersal mode for guppies, at least in the investigated stream. In the face of fluctuating environments, guppies seem to be extremely effective in keeping behavioral expressions constant, which could help explain their successful invasion and adaptation to disturbed habitats.},
	language = {English},
	urldate = {2021-06-05},
	journal = {Frontiers in Ecology and Evolution},
	author = {Lukas, Juliane and Kalinkat, Gregor and Miesen, Friedrich Wilhelm and Landgraf, Tim and Krause, Jens and Bierbach, David},
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {animal personality, Dispersal, Guppy, invasive species, range expansion, thermally altered freshwaters},
	file = {Full Text PDF:/Users/tim/Zotero/storage/EWKISKSK/Lukas et al. - 2021 - Consistent Behavioral Syndrome Across Seasons in a.pdf:application/pdf},
}

@article{doran_fish_2022,
	title = {Fish waves as emergent collective antipredator behavior},
	volume = {32},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982221016547},
	doi = {10.1016/j.cub.2021.11.068},
	abstract = {The collective behavior of animals has attracted considerable attention in recent years, with many studies exploring how local interactions between individuals can give rise to global group properties.1, 2, 3 The functional aspects of collective behavior are less well studied, especially in the field,4 and relatively few studies have investigated the adaptive benefits of collective behavior in situations where prey are attacked by predators.5,6 This paucity of studies is unsurprising because predator-prey interactions in the field are difficult to observe. Furthermore, the focus in recent studies on predator-prey interactions has been on the collective behavior of the prey7, 8, 9, 10 rather than on the behavior of the predator (but see Ioannou et al.11 and Handegard et al.12). Here we present a field study that investigated the anti-predator benefits of waves produced by fish at the water surface when diving down collectively in response to attacks of avian predators. Fish engaged in surface waves that were highly conspicuous, repetitive, and rhythmic involving many thousands of individuals for up to 2 min. Experimentally induced fish waves doubled the time birds waited until their next attack, therefore substantially reducing attack frequency. In one avian predator, capture probability, too, decreased with wave number and birds switched perches in response to wave displays more often than in control treatments, suggesting that they directed their attacks elsewhere. Taken together, these results support an anti-predator function of fish waves. The attack delay could be a result of a confusion effect or a consequence of waves acting as a perception advertisement, which requires further exploration.},
	language = {en},
	number = {3},
	urldate = {2022-04-19},
	journal = {Current Biology},
	author = {Doran, Carolina and Bierbach, David and Lukas, Juliane and Klamser, Pascal and Landgraf, Tim and Klenz, Haider and Habedank, Marie and Arias-Rodriguez, Lenin and Krause, Stefan and Romanczuk, Pawel and Krause, Jens},
	month = feb,
	year = {2022},
	pages = {708--714.e4},
	file = {ScienceDirect Snapshot:/Users/tim/Zotero/storage/DIM57D7F/S0960982221016547.html:text/html},
}

@article{smith_behavioral_2022,
	title = {Behavioral variation across the days and lives of honey bees},
	volume = {25},
	issn = {2589-0042},
	url = {https://www.sciencedirect.com/science/article/pii/S2589004222011142},
	doi = {10.1016/j.isci.2022.104842},
	abstract = {In honey bee colonies, workers generally change tasks with age (from brood care, to nest work, to foraging). While these trends are well established, our understanding of how individuals distribute tasks during a day, and how individuals differ in their lifetime behavioral trajectories, is limited. Here, we use automated tracking to obtain long-term data on 4,100+ bees tracked continuously at 3 Hz, across an entire summer, and use behavioral metrics to compare behavior at different timescales. Considering single days, we describe how bees differ in space use, detection, and movement. Analyzing the behavior exhibited across their entire lives, we find consistent inter-individual differences in the movement characteristics of individuals. Bees also differ in how quickly they transition through behavioral space to ultimately become foragers, with fast-transitioning bees living the shortest lives. Our analysis framework provides a quantitative approach to describe individual behavioral variation within a colony from single days to entire lifetimes.},
	language = {en},
	number = {9},
	urldate = {2022-08-22},
	journal = {iScience},
	author = {Smith, Michael L. and Davidson, Jacob D. and Wild, Benjamin and Dormagen, David M. and Landgraf, Tim and Couzin, Iain D.},
	month = sep,
	year = {2022},
	keywords = {Ethology, Methodology in biological sciences, Wildlife behavior},
	pages = {104842},
	file = {ScienceDirect Snapshot:/Users/tim/Zotero/storage/FJ3LDCX5/S2589004222011142.html:text/html},
}

@article{jolles_group-level_2020,
	title = {Group-level patterns emerge from individual speed as revealed by an extremely social robotic fish},
	volume = {16},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsbl.2020.0436},
	doi = {10.1098/rsbl.2020.0436},
	abstract = {Understanding the emergence of collective behaviour has long been a key research focus in the natural sciences. Besides the fundamental role of social interaction rules, a combination of theoretical and empirical work indicates individual speed may be a key process that drives the collective behaviour of animal groups. Socially induced changes in speed by interacting animals make it difficult to isolate the effects of individual speed on group-level behaviours. Here, we tackled this issue by pairing guppies with a biomimetic robot. We used a closed-loop tracking and feedback system to let a robotic fish naturally interact with a live partner in real time, and programmed it to strongly copy and follow its partner's movements while lacking any preferred movement speed or directionality of its own. We show that individual differences in guppies' movement speed were highly repeatable and in turn shaped key collective patterns: a higher individual speed resulted in stronger leadership, lower cohesion, higher alignment and better temporal coordination of the pairs. By combining the strengths of individual-based models and observational work with state-of-the-art robotics, we provide novel evidence that individual speed is a key, fundamental process in the emergence of collective behaviour.},
	number = {9},
	urldate = {2022-11-06},
	journal = {Biology Letters},
	author = {Jolles, Jolle W. and Weimar, Nils and Landgraf, Tim and Romanczuk, Pawel and Krause, Jens and Bierbach, David},
	month = sep,
	year = {2020},
	note = {Publisher: Royal Society},
	keywords = {individual differences, collective behaviour, robot, social, guppy, speed},
	pages = {20200436},
	file = {Full Text PDF:/Users/tim/Zotero/storage/CD7C8X49/Jolles et al. - 2020 - Group-level patterns emerge from individual speed .pdf:application/pdf},
}

@article{landgraf_animal---loop_2021,
	title = {Animal-in-the-{Loop}: {Using} {Interactive} {Robotic} {Conspecifics} to {Study} {Social} {Behavior} in {Animal} {Groups}},
	volume = {4},
	shorttitle = {Animal-in-the-{Loop}},
	url = {https://doi.org/10.1146/annurev-control-061920-103228},
	doi = {10.1146/annurev-control-061920-103228},
	abstract = {Biomimetic robots that replace living social interaction partners can help elucidate the underlying interaction rules in animal groups. Our review focuses on the use of interactive robots that respond dynamically to animal behavior as part of a closed control loop. We discuss the most influential works to date and how they have contributed to our understanding of animal sociality. Technological advances permit the use of robots that can adapt to the situations they face and the conspecifics they encounter, or robots that learn to optimize their social performance from a set of experiences. We discuss how adaptation and learning may provide novel insights into group sociobiology and describe the technical challenges associatedwith these types of interactive robots. This interdisciplinary field provides a rich set of problems to be tackled by roboticists, machine learning engineers, and control theorists. By cultivating smarter robots, we can usher in an era of more nuanced exploration of animal behavior.},
	number = {1},
	urldate = {2022-11-06},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Landgraf, Tim and Gebhardt, Gregor H.W. and Bierbach, David and Romanczuk, Pawel and Musiolek, Lea and Hafner, Verena V. and Krause, Jens},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-061920-103228},
	keywords = {reinforcement learning, biomimetic robots, interactive robots, animal-in-the-loop},
	pages = {487--507},
	file = {Full Text PDF:/Users/tim/Zotero/storage/N867YH2R/Landgraf et al. - 2021 - Animal-in-the-Loop Using Interactive Robotic Cons.pdf:application/pdf},
}

@article{bierbach_live_2022,
	title = {Live fish learn to anticipate the movement of a fish-like robot},
	volume = {17},
	issn = {1748-3190},
	url = {https://dx.doi.org/10.1088/1748-3190/ac8e3e},
	doi = {10.1088/1748-3190/ac8e3e},
	abstract = {The ability of an individual to predict the outcome of the actions of others and to change their own behavior adaptively is called anticipation. There are many examples from mammalian species—including humans—that show anticipatory abilities in a social context, however, it is not clear to what extent fishes can anticipate the actions of their interaction partners or what the underlying mechanisms are for that anticipation. To answer these questions, we let live guppies (Poecilia reticulata) interact repeatedly with an open-loop (noninteractive) biomimetic robot that has previously been shown to be an accepted conspecific. The robot always performed the same zigzag trajectory in the experimental tank that ended in one of the corners, giving the live fish the opportunity to learn both the location of the final destination as well as the specific turning movement of the robot over three consecutive trials. The live fish’s reactions were categorized into a global anticipation, which we defined as relative time to reach the robot’s final corner, and a local anticipation which was the relative time and location of the live fish’s turns relative to robofish turns. As a proxy for global anticipation, we found that live fish in the last trial reached the robot’s destination corner significantly earlier than the robot. Overall, more than 50\% of all fish arrived at the destination before the robot. This is more than a random walk model would predict and significantly more compared to all other equidistant, yet unvisited, corners. As a proxy for local anticipation, we found fish change their turning behavior in response to the robot over the course of the trials. Initially, the fish would turn after the robot, which was reversed in the end, as they began to turn slightly before the robot in the final trial. Our results indicate that live fish are able to anticipate predictably behaving social partners both in regard to final movement locations as well as movement dynamics. Given that fish have been found to exhibit consistent behavioral differences, anticipation in fish could have evolved as a mechanism to adapt to different social interaction partners.},
	language = {en},
	number = {6},
	urldate = {2022-11-06},
	journal = {Bioinspiration \& Biomimetics},
	author = {Bierbach, David and Gómez-Nava, Luis and Francisco, Fritz A. and Lukas, Juliane and Musiolek, Lea and Hafner, Verena V. and Landgraf, Tim and Romanczuk, Pawel and Krause, Jens},
	month = oct,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {065007},
	file = {IOP Full Text PDF:/Users/tim/Zotero/storage/FTN6XNKN/Bierbach et al. - 2022 - Live fish learn to anticipate the movement of a fi.pdf:application/pdf},
}

@article{klamser_impact_2021,
	title = {Impact of {Variable} {Speed} on {Collective} {Movement} of {Animal} {Groups}},
	volume = {9},
	issn = {2296-424X},
	url = {https://www.frontiersin.org/articles/10.3389/fphy.2021.715996},
	abstract = {The collective dynamics and structure of animal groups has attracted the attention of scientists across a broad range of fields. A variety of agent-based models have been developed to help understand the emergence of coordinated collective behavior from simple interaction rules. A common, simplifying assumption of such collective movement models, is that individual agents move with a constant speed. In this work we critically re-asses this assumption. First, we discuss experimental data showcasing the omnipresent speed variability observed in different species of live fish and artificial agents (RoboFish). Based on theoretical considerations accounting for inertia and rotational friction, we derive a functional dependence of the turning response of individuals on their instantaneous speed, which is confirmed by experimental data. We then investigate the interplay of variable speed and speed-dependent turning on self-organized collective behavior by implementing an agent-based model which accounts for both these effects. We show that, besides the average speed of individuals, the variability in individual speed can have a dramatic impact on the emergent collective dynamics: a group which differs to another only in a lower speed variability of its individuals (groups being identical in all other behavioral parameters), can be in the polarized state while the other group is disordered. We find that the local coupling between group polarization and individual speed is strongest at the order-disorder transition, and that, in contrast to fixed speed models, the group’s spatial extent does not have a maximum at the transition. Furthermore, we demonstrate a decrease in polarization with group size for groups of individuals with variable speed, and a sudden decrease in mean individual speed at a critical group size (N = 4 for Voronoi interactions) linked to a topological transition from an all-to-all to a distributed spatial interaction network. Overall, our work highlights the importance to account for fundamental kinematic constraints in general, and variable speed in particular, when modeling self-organized collective dynamics.},
	urldate = {2022-11-06},
	journal = {Frontiers in Physics},
	author = {Klamser, Pascal P. and Gómez-Nava, Luis and Landgraf, Tim and Jolles, Jolle W. and Bierbach, David and Romanczuk, Pawel},
	year = {2021},
	file = {Full Text PDF:/Users/tim/Zotero/storage/KND28BRP/Klamser et al. - 2021 - Impact of Variable Speed on Collective Movement of.pdf:application/pdf},
}

@inproceedings{nader_dnnr_2022,
	title = {{DNNR}: {Differential} {Nearest} {Neighbors} {Regression}},
	shorttitle = {{DNNR}},
	url = {https://proceedings.mlr.press/v162/nader22a.html},
	abstract = {K-nearest neighbors (KNN) is one of the earliest and most established algorithms in machine learning. For regression tasks, KNN averages the targets within a neighborhood which poses a number of challenges: the neighborhood definition is crucial for the predictive performance as neighbors might be selected based on uninformative features, and averaging does not account for how the function changes locally. We propose a novel method called Differential Nearest Neighbors Regression (DNNR) that addresses both issues simultaneously: during training, DNNR estimates local gradients to scale the features; during inference, it performs an n-th order Taylor approximation using estimated gradients. In a large-scale evaluation on over 250 datasets, we find that DNNR performs comparably to state-of-the-art gradient boosting methods and MLPs while maintaining the simplicity and transparency of KNN. This allows us to derive theoretical error bounds and inspect failures. In times that call for transparency of ML models, DNNR provides a good balance between performance and interpretability.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nader, Youssef and Sixt, Leon and Landgraf, Tim},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16296--16317},
	file = {Full Text PDF:/Users/tim/Zotero/storage/N47WF5DZ/Nader et al. - 2022 - DNNR Differential Nearest Neighbors Regression.pdf:application/pdf},
}

@inproceedings{schulz_restricting_2020,
	title = {Restricting the {Flow}: {Information} {Bottlenecks} for {Attribution}},
	shorttitle = {Restricting the {Flow}},
	url = {https://openreview.net/forum?id=S1xWh1rYwB},
	abstract = {Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work, we adopt the information bottleneck concept for attribution. By adding noise to intermediate feature maps, we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} {Representations}},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} {Representations}},
	author = {Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
	month = may,
	year = {2020},
	file = {Full Text PDF:/Users/tim/Zotero/storage/82BZYCA6/Schulz et al. - 2020 - Restricting the Flow Information Bottlenecks for .pdf:application/pdf},
}

@inproceedings{sixt_users_2022,
	title = {Do {Users} {Benefit} {From} {Interpretable} {Vision}? {A} {User} {Study}, {Baseline}, {And} {Dataset}},
	shorttitle = {Do {Users} {Benefit} {From} {Interpretable} {Vision}?},
	url = {https://openreview.net/forum?id=v6s3HVjPerv},
	abstract = {A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} {Representations}},
	booktitle = {Proceedings of the {International} {Conference} on {Learning} {Representations}},
	author = {Sixt, Leon and Schuessler, Martin and Popescu, Oana-Iuliana and Weiß, Philipp and Landgraf, Tim},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tim/Zotero/storage/Q9A2IPF7/Sixt et al. - 2022 - Do Users Benefit From Interpretable Vision A User.pdf:application/pdf},
}

@misc{boenisch_tracking_2018-1,
	title = {Tracking all members of a honey bee colony over their lifetime},
	url = {http://arxiv.org/abs/1802.03192},
	doi = {10.48550/arXiv.1802.03192},
	abstract = {Computational approaches to the analysis of collective behavior in social insects increasingly rely on motion paths as an intermediate data layer from which one can infer individual behaviors or social interactions. Honey bees are a popular model for learning and memory. Previous experience has been shown to affect and modulate future social interactions. So far, no lifetime history observations have been reported for all bees of a colony. In a previous work we introduced a tracking system customized to track up to \$4000\$ bees over several weeks. In this contribution we present an in-depth description of the underlying multi-step algorithm which both produces the motion paths, and also improves the marker decoding accuracy significantly. We automatically tracked \$\{{\textbackslash}sim\}2000\$ marked honey bees over 10 weeks with inexpensive recording hardware using markers without any error correction bits. We found that the proposed two-step tracking reduced incorrect ID decodings from initially \$\{{\textbackslash}sim\}13{\textbackslash}\%\$ to around \$2{\textbackslash}\%\$ post-tracking. Alongside this paper, we publish the first trajectory dataset for all bees in a colony, extracted from \$\{{\textbackslash}sim\} 4\$ million images. We invite researchers to join the collective scientific effort to investigate this intriguing animal system. All components of our system are open-source.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Boenisch, Franziska and Rosemann, Benjamin and Wild, Benjamin and Wario, Fernando and Dormagen, David and Landgraf, Tim},
	month = mar,
	year = {2018},
	note = {arXiv:1802.03192 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/FRN99YPZ/Boenisch et al. - 2018 - Tracking all members of a honey bee colony over th.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/GDNRWPUJ/1802.html:text/html},
}

@misc{wild_automatic_2018,
	title = {Automatic localization and decoding of honeybee markers using deep convolutional neural networks},
	url = {http://arxiv.org/abs/1802.04557},
	doi = {10.48550/arXiv.1802.04557},
	abstract = {The honeybee is a fascinating model animal to investigate how collective behavior emerges from (inter-)actions of thousands of individuals. Bees may acquire unique memories throughout their lives. These experiences affect social interactions even over large time frames. Tracking and identifying all bees in the colony over their lifetimes therefore may likely shed light on the interplay of individual differences and colony behavior. This paper proposes a software pipeline based on two deep convolutional neural networks for the localization and decoding of custom binary markers that honeybees carry from their first to the last day in their life. We show that this approach outperforms similar systems proposed in recent literature. By opening this software for the public, we hope that the resulting datasets will help advancing the understanding of honeybee collective intelligence.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Wild, Benjamin and Sixt, Leon and Landgraf, Tim},
	month = feb,
	year = {2018},
	note = {arXiv:1802.04557 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/CK58ZHIE/Wild et al. - 2018 - Automatic localization and decoding of honeybee ma.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/DGTYD5GA/1802.html:text/html},
}

@misc{landgraf_dancing_2018,
	title = {Dancing {Honey} bee {Robot} {Elicits} {Dance}-{Following} and {Recruits} {Foragers}},
	url = {http://arxiv.org/abs/1803.07126},
	doi = {10.48550/arXiv.1803.07126},
	abstract = {The honey bee dance communication system is one of the most popular examples of animal communication. Forager bees communicate the flight vector towards food, water, or resin sources to nestmates by performing a stereotypical motion pattern on the comb surface in the darkness of the hive. Bees that actively follow the circles of the dancer, so called dance-followers, may decode the message and fly according to the indicated vector that refers to the sun compass and their visual odometer. We investigated the dance communication system with a honeybee robot that reproduced the waggle dance pattern for a flight vector chosen by the experimenter. The dancing robot, called RoboBee, generated multiple cues contained in the biological dance pattern and elicited natural dance-following behavior in live bees. By tracking the flight trajectory of departing bees after following the dancing robot via harmonic radar we confirmed that bees used information obtained from the robotic dance to adjust their flight path. This is the first report on successful dance following and subsequent flight performance of bees recruited by a biomimetic robot.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Landgraf, Tim and Bierbach, David and Kirbach, Andreas and Cusing, Rachel and Oertel, Michael and Lehmann, Konstantin and Greggers, Uwe and Menzel, Randolf and Rojas, Raúl},
	month = mar,
	year = {2018},
	note = {arXiv:1803.07126 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/9RJ79V5U/Landgraf et al. - 2018 - Dancing Honey bee Robot Elicits Dance-Following an.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/NM8RFXP5/1803.html:text/html},
}

@misc{monck_biotracker_2018,
	title = {{BioTracker}: {An} {Open}-{Source} {Computer} {Vision} {Framework} for {Visual} {Animal} {Tracking}},
	shorttitle = {{BioTracker}},
	url = {http://arxiv.org/abs/1803.07985},
	doi = {10.48550/arXiv.1803.07985},
	abstract = {The study of animal behavior increasingly relies on (semi-) automatic methods for the extraction of relevant behavioral features from video or picture data. To date, several specialized software products exist to detect and track animals' positions in simple (laboratory) environments. Tracking animals in their natural environments, however, often requires substantial customization of the image processing algorithms to the problem-specific image characteristics. Here we introduce BioTracker, an open-source computer vision framework, that provides programmers with core functionalities that are essential parts of a tracking software, such as video I/O, graphics overlays and mouse and keyboard interfaces. BioTracker additionally provides a number of different tracking algorithms suitable for a variety of image recording conditions. The main feature of BioTracker is however the straightforward implementation of new problem-specific tracking modules and vision algorithms that can build upon BioTracker's core functionalities. With this open-source framework the scientific community can accelerate their research and focus on the development of new vision algorithms.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Mönck, Hauke Jürgen and Jörg, Andreas and von Falkenhausen, Tobias and Tanke, Julian and Wild, Benjamin and Dormagen, David and Piotrowski, Jonas and Winklmayr, Claudia and Bierbach, David and Landgraf, Tim},
	month = mar,
	year = {2018},
	note = {arXiv:1803.07985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/L9MXMG99/Mönck et al. - 2018 - BioTracker An Open-Source Computer Vision Framewo.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/7FCTUMGE/1803.html:text/html},
}

@misc{polster_reconstructing_2019,
	title = {Reconstructing the visual perception of honey bees in complex 3-{D} worlds},
	url = {http://arxiv.org/abs/1811.07560},
	doi = {10.48550/arXiv.1811.07560},
	abstract = {Over the last decades, honeybees have been a fascinating model to study insect navigation. While there is some controversy about the complexity of underlying neural correlates, the research of honeybee navigation makes progress through both the analysis of flight behavior and the synthesis of agent models. Since visual cues are believed to play a crucial role for the behavioral output of a navigating bee we have developed a realistic 3-dimensional virtual world, in which simulated agents can be tested, or in which the visual input of experimentally traced animals can be reconstructed. In this paper we present implementation details on how we reconstructed a large 3-dimensional world from aerial imagery of one of our field sites, how the distribution of ommatidia and their view geometry was modeled, and how the system samples from the scene to obtain realistic bee views. This system is made available as an open-source project to the community on {\textbackslash}url\{http://github.com/bioroboticslab/bee\_view\}.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Polster, Johannes and Petrasch, Julian and Menzel, Randolf and Landgraf, Tim},
	month = jun,
	year = {2019},
	note = {arXiv:1811.07560 [q-bio]},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/MFM8U222/Polster et al. - 2019 - Reconstructing the visual perception of honey bees.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/KT8AVLSD/1811.html:text/html},
}

@misc{schulz_restricting_2020-1,
	title = {Restricting the {Flow}: {Information} {Bottlenecks} for {Attribution}},
	shorttitle = {Restricting the {Flow}},
	url = {http://arxiv.org/abs/2001.00396},
	doi = {10.48550/arXiv.2001.00396},
	abstract = {Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method's information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB For code: https://github.com/BioroboticsLab/IBA},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
	month = may,
	year = {2020},
	note = {arXiv:2001.00396 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/ASXYC3XC/Schulz et al. - 2020 - Restricting the Flow Information Bottlenecks for .pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/GDFJBPUY/2001.html:text/html},
}

@misc{landgraf_socially_2020,
	title = {Socially competent robots: adaptation improves leadership performance in groups of live fish},
	shorttitle = {Socially competent robots},
	url = {http://arxiv.org/abs/2009.06633},
	doi = {10.48550/arXiv.2009.06633},
	abstract = {Collective motion is commonly modeled with simple interaction rules between agents. Yet in nature, numerous observables vary within and between individuals and it remains largely unknown how animals respond to this variability, and how much of it may be the result of social responses. Here, we hypothesize that Guppies ({\textbackslash}textit\{Poecilia reticulata\}) respond to avoidance behaviors of their shoal mates and that "socially competent" responses allow them to be more effective leaders. We test this hypothesis in an experimental setting in which a robotic Guppy, called RoboFish, is programmed to adapt to avoidance reactions of its live interaction partner. We compare the leadership performance between socially competent robots and two non-competent control behaviors and find that 1) behavioral variability itself appears attractive and that socially competent robots are better leaders that 2) require fewer approach attempts to 3) elicit longer average following behavior than non-competent agents. This work provides evidence that social responsiveness to avoidance reactions plays a role in the social dynamics of guppies. We showcase how social responsiveness can be modeled and tested directly embedded in a living animal model using adaptive, interactive robots.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Landgraf, Tim and Moenck, Hauke J. and Gebhardt, Gregor H. W. and Weimar, Nils and Hocke, Mathis and Maxeiner, Moritz and Musiolek, Lea and Krause, Jens and Bierbach, David},
	month = sep,
	year = {2020},
	note = {arXiv:2009.06633 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/7LXHVWLH/Landgraf et al. - 2020 - Socially competent robots adaptation improves lea.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/ICP5GW7H/2009.html:text/html},
}

@misc{smith_dominant_2021,
	title = {The dominant axes of lifetime behavioral variation in honey bees},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.04.15.440020v1},
	doi = {10.1101/2021.04.15.440020},
	abstract = {Insect colonies are decentralized systems that employ task allocation, whereby individuals undertake different roles to fulfil colony needs, such as honey bee “nurses”, “nest workers”, and “foragers”. However, the extent to which individuals can be well-classified by discrete “roles”, how they change behavior from day-to-day, over entire lifetimes, and with environmental conditions, is poorly understood. Using long-term automated tracking of over 4,200 individually-identified bees Apis mellifera, we use behavioral metrics to quantify and compare behavior. We show that individuals exhibit behavioral variation along two dominant axes that represent nest substrate use and movement within the nest. Across lifetimes, we find that individuals differ in foraging onset, and that certain bees exhibit lifelong consistencies in their movement patterns. Furthermore, we examine a period of sudden nectar availability where the honey stores tripled over 6 days, and see that the colony exhibits a distributed shift in activity that did not require a large-scale colony reorganization. Our quantitative approach shows how collective units differ over days and lifetimes, and how sources of variation and variability contribute to the colony’s robust yet flexible response.},
	language = {en},
	urldate = {2023-01-09},
	publisher = {bioRxiv},
	author = {Smith, Michael L. and Davidson, Jacob D. and Wild, Benjamin and Dormagen, David M. and Landgraf, Tim and Couzin, Iain D.},
	month = apr,
	year = {2021},
	note = {Pages: 2021.04.15.440020
Section: New Results},
	file = {Full Text PDF:/Users/tim/Zotero/storage/5JU2M5HC/Smith et al. - 2021 - The dominant axes of lifetime behavioral variation.pdf:application/pdf},
}

@misc{wild_learning_2021,
	title = {Learning to embed lifetime social behavior from interaction dynamics},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.09.01.458538v1},
	doi = {10.1101/2021.09.01.458538},
	abstract = {Interactions of individuals in complex social systems give rise to emergent behaviors at the group level. Identifying the functional role that individuals take in the group at a specific time facilitates understanding the dynamics of these emergent processes. An individual’s behavior at a given time can be partially inferred by common factors, such as age, but internal and external factors also substantially influence behavior, making it difficult to disentangle common development from individuality. Here we show that such dependencies on common factors can be used as an implicit bias to learn a temporally consistent representation of a functional role from social interaction networks. Using a unique dataset containing lifetime trajectories of multiple generations of individually-marked honey bees in two colonies, we propose a new temporal matrix factorization model that jointly learns the average developmental path and structured variations of individuals in the social network over their entire lives. Our method yields inherently interpretable embeddings that are biologically relevant and consistent over time, allowing one to compare individuals’ functional roles regardless of when or in which colony they lived. Our method provides a quantitative framework for understanding behavioral heterogeneity in complex social systems, and is applicable to fields such as behavioral biology, social sciences, neuroscience, and information science.
Author summary Group-level emergent behaviors are the result of interactions between individual group members. To understand these social dynamics, one must objectively measure the function of an individual in their group at any given time. Ideally, one would also like to compare individuals from different groups, for example, to measure how specific environmental conditions or other external factors influence group behavior. Unfortunately, such an objective measure is hard to obtain because the group and its dynamics constantly change, making it challenging to define an individual’s role in the group as a function of its actions and interactions. We propose a principled approach to model individuals in complex social systems by considering that function often depends, at least partially, on common factors such as age. The model learns a meaningful and interpretable descriptor for all individuals, and can be used to understand how complex social systems function and the emergence of group behavior.},
	language = {en},
	urldate = {2023-01-09},
	publisher = {bioRxiv},
	author = {Wild, Benjamin and Dormagen, David M. and Smith, Michael L. and Landgraf, Tim},
	month = sep,
	year = {2021},
	note = {Pages: 2021.09.01.458538
Section: New Results},
	file = {Full Text PDF:/Users/tim/Zotero/storage/UMUHCJAL/Wild et al. - 2021 - Learning to embed lifetime social behavior from in.pdf:application/pdf},
}

@misc{klamser_impact_2021-1,
	title = {Impact of {Variable} {Speed} on {Collective} {Movement} of {Animal} {Groups}},
	url = {http://arxiv.org/abs/2106.00959},
	doi = {10.48550/arXiv.2106.00959},
	abstract = {A variety of agent-based models has been proposed to account for the emergence of coordinated collective behavior of animal groups from simple interaction rules. A common, simplifying assumption of such collective movement models, is the consideration of individual agents moving with a constant speed. In this work we critically re-asses this assumption underlying a vast majority of collective movement models. First, we show the omnipresent speed variability observed in different species of live fish and artificial agents (RoboFish). Based on theoretical considerations accounting for inertia and rotational friction, we derive a functional dependence of the turning response of individuals on their instantaneous speed (confirmed by experimental data). We investigate how the interplay of variable speed and speed-dependent turning affects self-organized collective behavior by implementing an agent-based model which accounts for both effects. We show, that besides average speed, the individual speed variability may have a dramatic impact on the emergent collective dynamics, as two groups differing only in their speed variability, and being otherwise identical in all other behavioral parameters, can be in two fundamentally different stationary states. We find that the local coupling between group polarization and individual speed is strongest at the order-disorder transition. Furthermore, we demonstrate a decrease in polarization with group size for groups of individuals with variable speed, and a sudden decrease in mean individual speed at a critical group size (N=4 for Voronoi interactions) linked to a topological transition from an all-to-all to a distributed spatial interaction network. Overall, our work highlights the importance to account for fundamental kinematic constraints in general, and variable speed in particular, when modeling self-organized collective dynamics.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Klamser, Pascal P. and Gómez-Nava, Luis and Landgraf, Tim and Jolles, Jolle W. and Bierbach, David and Romanczuk, Pawel},
	month = jun,
	year = {2021},
	note = {arXiv:2106.00959 [physics, q-bio]},
	keywords = {Quantitative Biology - Populations and Evolution, Physics - Biological Physics},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/FXIVNU3B/Klamser et al. - 2021 - Impact of Variable Speed on Collective Movement of.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/4F9GZ8RM/2106.html:text/html},
}

@inproceedings{solopova_german_2021,
	address = {National Institute of Technology Silchar, Silchar, India},
	title = {A {German} {Corpus} of {Reflective} {Sentences}},
	url = {https://aclanthology.org/2021.icon-main.72},
	abstract = {Reflection about a learning process is beneficial to students in higher education (Bub-nys, 2019). The importance of machine understanding of reflective texts grows as applications supporting students become more widespread. Nevertheless, due to the sensitive content, there is no public corpus available yet for the classification of text reflectiveness. We provide the first open-access corpus of reflective student essays in German. We collected essays from three different disciplines (Software Development, Ethics of Artificial Intelligence, and Teacher Training). We annotated the corpus at sentence level with binary reflective/non-reflective labels, using an iterative annotation process with linguistic and didactic specialists, mapping the reflective components found in the data to existing schemes and complementing them. We propose and evaluate linguistic features of reflectiveness and analyse their distribution within the resulted sentences according to their labels. Our contribution constitutes the first open-access corpus to help the community towards a unified approach for reflection detection.},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Natural} {Language} {Processing} ({ICON})},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Natural} {Language} {Processing} ({ICON})},
	publisher = {NLP Association of India (NLPAI)},
	author = {Solopova, Veronika and Popescu, Oana-Iuliana and Chikobava, Margarita and Romeike, Ralf and Landgraf, Tim and Benzmüller, Christoph},
	month = dec,
	year = {2021},
	pages = {593--600},
	file = {Full Text PDF:/Users/tim/Zotero/storage/ESM4YWG2/Solopova et al. - 2021 - A German Corpus of Reflective Sentences.pdf:application/pdf},
}

@inproceedings{herrmann_chaotic_2022,
	title = {Chaotic {Dynamics} are {Intrinsic} to {Neural} {Network} {Training} with {SGD}},
	url = {https://openreview.net/forum?id=ffy-h0GKZbK},
	abstract = {With the advent of deep learning over the last decade, a considerable amount of effort has gone into better understanding and enhancing Stochastic Gradient Descent so as to improve the performance and stability of artificial neural network training. Active research fields in this area include exploiting second order information of the loss landscape and improving the understanding of chaotic dynamics in optimization. This paper exploits the theoretical connection between the curvature of the loss landscape and chaotic dynamics in neural network training to propose a modified SGD ensuring non-chaotic training dynamics to study the importance thereof in NN training. Building on this, we present empirical evidence suggesting that the negative eigenspectrum - and thus directions of local chaos - cannot be removed from SGD without hurting training performance. Extending our empirical analysis to long-term chaos dynamics, we challenge the widespread understanding of convergence against a confined region in parameter space. Our results show that although chaotic network behavior is mostly confined to the initial training phase, models perturbed upon initialization do diverge at a slow pace even after reaching top training performance, and that their divergence can be modelled through a composition of a random walk and a linear divergence. The tools and insights developed as part of our work contribute to improving the understanding of neural network training dynamics and provide a basis for future improvements of optimization methods.},
	language = {en},
	urldate = {2023-01-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Herrmann, Luis and Granz, Maximilian and Landgraf, Tim},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/tim/Zotero/storage/J67NT4MN/Herrmann et al. - 2022 - Chaotic Dynamics are Intrinsic to Neural Network T.pdf:application/pdf},
}

@misc{sixt_users_2022-1,
	title = {Do {Users} {Benefit} {From} {Interpretable} {Vision}? {A} {User} {Study}, {Baseline}, {And} {Dataset}},
	shorttitle = {Do {Users} {Benefit} {From} {Interpretable} {Vision}?},
	url = {http://arxiv.org/abs/2204.11642},
	doi = {10.48550/arXiv.2204.11642},
	abstract = {A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model's respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies. For code see, https://github.com/berleon/do\_users\_benefit\_from\_interpretable\_vision},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Sixt, Leon and Schuessler, Martin and Popescu, Oana-Iuliana and Weiß, Philipp and Landgraf, Tim},
	month = apr,
	year = {2022},
	note = {arXiv:2204.11642 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/R7NGNA2C/Sixt et al. - 2022 - Do Users Benefit From Interpretable Vision A User.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/KJM3JUJT/2204.html:text/html},
}

@misc{nader_dnnr_2022-1,
	title = {{DNNR}: {Differential} {Nearest} {Neighbors} {Regression}},
	shorttitle = {{DNNR}},
	url = {http://arxiv.org/abs/2205.08434},
	doi = {10.48550/arXiv.2205.08434},
	abstract = {K-nearest neighbors (KNN) is one of the earliest and most established algorithms in machine learning. For regression tasks, KNN averages the targets within a neighborhood which poses a number of challenges: the neighborhood definition is crucial for the predictive performance as neighbors might be selected based on uninformative features, and averaging does not account for how the function changes locally. We propose a novel method called Differential Nearest Neighbors Regression (DNNR) that addresses both issues simultaneously: during training, DNNR estimates local gradients to scale the features; during inference, it performs an n-th order Taylor approximation using estimated gradients. In a large-scale evaluation on over 250 datasets, we find that DNNR performs comparably to state-of-the-art gradient boosting methods and MLPs while maintaining the simplicity and transparency of KNN. This allows us to derive theoretical error bounds and inspect failures. In times that call for transparency of ML models, DNNR provides a good balance between performance and interpretability.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Nader, Youssef and Sixt, Leon and Landgraf, Tim},
	month = may,
	year = {2022},
	note = {arXiv:2205.08434 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/KUX5ML3C/Nader et al. - 2022 - DNNR Differential Nearest Neighbors Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/SQ36VZBI/2205.html:text/html},
}

@misc{sixt_rigorous_2022,
	title = {A {Rigorous} {Study} {Of} {The} {Deep} {Taylor} {Decomposition}},
	url = {http://arxiv.org/abs/2211.08425},
	doi = {10.48550/arXiv.2211.08425},
	abstract = {Saliency methods attempt to explain deep neural networks by highlighting the most salient features of a sample. Some widely used methods are based on a theoretical framework called Deep Taylor Decomposition (DTD), which formalizes the recursive application of the Taylor Theorem to the network's layers. However, recent work has found these methods to be independent of the network's deeper layers and appear to respond only to lower-level image structure. Here, we investigate the DTD theory to better understand this perplexing behavior and found that the Deep Taylor Decomposition is equivalent to the basic gradient\${\textbackslash}times\$input method when the Taylor root points (an important parameter of the algorithm chosen by the user) are locally constant. If the root points are locally input-dependent, then one can justify any explanation. In this case, the theory is under-constrained. In an empirical evaluation, we find that DTD roots do not lie in the same linear regions as the input - contrary to a fundamental assumption of the Taylor theorem. The theoretical foundations of DTD were cited as a source of reliability for the explanations. However, our findings urge caution in making such claims.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Sixt, Leon and Landgraf, Tim},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08425 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/UUB4ALUS/Sixt and Landgraf - 2022 - A Rigorous Study Of The Deep Taylor Decomposition.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/T5VNN5CI/2211.html:text/html},
}

@misc{neubauer_honey_2023,
	title = {Honey bee drones are synchronously hyperactive inside the nest},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.01.19.524638v1},
	doi = {10.1101/2023.01.19.524638},
	abstract = {1 Abstract
Eusocial insects operate as an integrated collective with tasks allocated among individuals. This applies also to reproduction, through coordinated mating flights between male and female reproductives. While in some species male sexuals take only a single mating flight and never return, in the Western honey bee, Apis mellifera, the male sexuals (drones) live in the colony throughout their lives. Prior research has focused almost exclusively on drone behavior outside of the nest (mating flights), while ignoring the majority of their life, which is spent inside the nest. To understand the in-nest behavior of drones across their lives, we used the BeesBook tracking system to track 192 individually-marked drones continuously for over 20 days, to examine how drones moved and spent time in the nest. In agreement with previous work, we find that drones spend most of their time immobile at the nest periphery. However, we also observe that drones have periods of in-nest hyperactivity, during which they become the most active individuals in the entire colony. This in-nest hyperactivity develops in drones after age 7 days, occurs daily in the afternoon, and coincides with drones taking outside trips. We find strong synchronization across the drones in the start/end of activity, such that the drones in the colony exhibit a “shared activation period”. The duration of the shared activation period depends on the weather; when conditions are suitable for mating flights, the activation period is extended. At the individual-level, we see that the activation order changes from day to day, suggesting that both the external influence of weather conditions, as well as exchange of social information, influences individual activation. Using an accumulation-to-threshold model of drone activation, we show that simulations using social information match experimental observations. These results provide new insight into the in-nest behavior of drones, and how their behavior reflects their role as the male gametes of the colony.},
	language = {en},
	urldate = {2023-03-05},
	publisher = {bioRxiv},
	author = {Neubauer, Louisa C. and Davidson, Jacob D. and Wild, Benjamin and Dormagen, David M. and Landgraf, Tim and Couzin, Iain D. and Smith, Michael L.},
	month = jan,
	year = {2023},
	note = {Pages: 2023.01.19.524638
Section: New Results},
	file = {Full Text PDF:/Users/tim/Zotero/storage/KFKLNZV4/Neubauer et al. - 2023 - Honey bee drones are synchronously hyperactive ins.pdf:application/pdf},
}

@article{landgraf_data_2023,
	title = {Data for the publication "{Socially} competent robots"},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://refubium.fu-berlin.de/handle/fub188/36717},
	doi = {10.17169/refubium-36430},
	abstract = {Raw tracking data and metadata.},
	language = {eng},
	urldate = {2023-03-05},
	author = {Landgraf, Tim and Bierbach, David and Moenck, Hauke J. and Musiolek, Lea and Hocke, Mathis and Maxeiner, Moritz},
	year = {2023},
	note = {Accepted: 2023-02-14T08:50:59Z
Publisher: Freie Universität Berlin
Type: dataset},
}

@misc{solopova_automated_2023,
	title = {Automated multilingual detection of {Pro}-{Kremlin} propaganda in newspapers and {Telegram} posts},
	url = {http://arxiv.org/abs/2301.10604},
	doi = {10.48550/arXiv.2301.10604},
	abstract = {The full-scale conflict between the Russian Federation and Ukraine generated an unprecedented amount of news articles and social media data reflecting opposing ideologies and narratives. These polarized campaigns have led to mutual accusations of misinformation and fake news, shaping an atmosphere of confusion and mistrust for readers worldwide. This study analyses how the media affected and mirrored public opinion during the first month of the war using news articles and Telegram news channels in Ukrainian, Russian, Romanian and English. We propose and compare two methods of multilingual automated pro-Kremlin propaganda identification, based on Transformers and linguistic features. We analyse the advantages and disadvantages of both methods, their adaptability to new genres and languages, and ethical considerations of their usage for content moderation. With this work, we aim to lay the foundation for further development of moderation tools tailored to the current conflict.},
	urldate = {2023-03-05},
	publisher = {arXiv},
	author = {Solopova, Veronika and Popescu, Oana-Iuliana and Benzmüller, Christoph and Landgraf, Tim},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10604 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tim/Zotero/storage/QHP8JZAM/Solopova et al. - 2023 - Automated multilingual detection of Pro-Kremlin pr.pdf:application/pdf;arXiv.org Snapshot:/Users/tim/Zotero/storage/GHV9U3JI/2301.html:text/html},
}
